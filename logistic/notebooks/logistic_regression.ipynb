{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will implement a logistic regression gradient descent algorithm;\n",
    "## To find a w which minimizes the negative log likelihood of the data.\n",
    "\n",
    "### Logistic Regression basis function\n",
    "\n",
    "The logistic regression model is a linear model for binary classification. It is based on the logistic function, which is defined as:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Where $z = w^T x$ is the linear combination of the input features $x$ and the weights $w$. The logistic function maps the linear combination to the range $[0, 1]$, which can be interpreted as a probability.\n",
    "\n",
    "### Logistic regression probabilistic interpretation\n",
    "\n",
    "The logistic regression model can be interpreted as a probabilistic model. The probability of the target variable $y$ being 1 given the input features $x$ is given by:\n",
    "\n",
    "$$p(y=1|x, w) = \\sigma(w^T x)$$\n",
    "\n",
    "The probability of the target variable $y$ being 0 given the input features $x$ is given by:\n",
    "\n",
    "$$p(y=0|x, w) = 1 - \\sigma(w^T x)$$\n",
    "\n",
    "### Negative log likelihood\n",
    "\n",
    "The negative log likelihood of the data given the weights $w$ is given by:\n",
    "\n",
    "$$\\mathcal{L}(w) = -\\sum_{i=1}^{N} y_i \\log(\\sigma(w^T x_i)) + (1 - y_i) \\log(1 - \\sigma(w^T x_i))$$\n",
    "\n",
    "Where $N$ is the number of samples in the dataset, $x_i$ is the input features of the $i$-th sample, and $y_i$ is the target variable of the $i$-th sample.\n",
    "\n",
    "### Gradient of the negative log likelihood\n",
    "\n",
    "The gradient of the negative log likelihood with respect to the weights $w$ is given by:\n",
    "\n",
    "$$\\nabla \\mathcal{L}(w) = -\\sum_{i=1}^{N} (y_i - \\sigma(w^T x_i)) x_i$$\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "The weights $w$ are updated using the gradient descent algorithm. The update rule is given by:\n",
    "\n",
    "$$w = w - \\alpha \\nabla \\mathcal{L}(w)$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "In the stochastic gradient descent algorithm, the weights are updated using the gradient of the negative log likelihood of a single sample at each iteration. The update rule is given by:\n",
    "\n",
    "$$w = w - \\alpha (y_i - \\sigma(w^T x_i)) x_i$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate, $x_i$ is the input features of the $i$-th sample, and $y_i$ is the target variable of the $i$-th sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "The negative log likelihood can be regularized to prevent overfitting. The regularized negative log likelihood is given by:\n",
    "\n",
    "$$\\mathcal{L}(w) = -\\sum_{i=1}^{N} y_i \\log(\\sigma(w^T x_i)) + (1 - y_i) \\log(1 - \\sigma(w^T x_i)) + \\frac{\\lambda}{2} ||w||^2$$\n",
    "\n",
    "Where $\\lambda$ is the regularization parameter.\n",
    "\n",
    "The gradient of the regularized negative log likelihood with respect to the weights $w$ is given by:\n",
    "\n",
    "$$\\nabla \\mathcal{L}(w) = -\\sum_{i=1}^{N} (y_i - \\sigma(w^T x_i)) x_i + \\lambda w$$\n",
    "\n",
    "The weights are updated using the gradient descent algorithm. The update rule is given by:\n",
    "\n",
    "$$w = w - \\alpha \\nabla \\mathcal{L}(w)$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
